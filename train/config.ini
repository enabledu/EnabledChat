[MODEL_ARGUMENTS]
model_name = "decapoda-research/llama-7b-hf"
tokenizer_name = "decapoda-research/llama-7b-hf"
use_auth_token = False

[DATA_TRAINING_ARGUMENTS]
input_column = "input"
target_column = None
dataset_name = "0x70DA/chat-data"
dataset_config_name = None
preprocessing_num_workers = 1000
max_source_length = 512
max_target_length = 128
val_max_target_length = 128
pad_to_max_length = True
ignore_pad_token_for_loss = True
max_train_samples = 1000
max_eval_samples = 50

[TRAINING_ARGUMENTS]
output_dir = "checkpoints/llama-7b-hf"
overwrite_output_dir = False
evaluation_strategy = "steps"
per_device_train_batch_size = 32
per_device_eval_batch_size = 32
batch_size = 64
warmup_steps = 100
num_train_epochs = 1
learning_rate = 0.0002
fp16 = True
logging_steps = 20
save_strategy = "steps"
eval_steps = 200
save_steps = 200
save_total_limit = 100
load_best_model_at_end = True

[LORA_CONFIG]
lora_r = 8
lora_alpha = 16
lora_dropout = 0.05
target_modules = "q_proj,k_proj,v_proj,down_proj,gate_proj,up_proj"