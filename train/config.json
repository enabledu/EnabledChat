{
    "model_name": "decapoda-research/llama-7b-hf",
    "tokenizer_name": "hf-internal-testing/llama-tokenizer",
    "use_auth_token": false,
    "use_fast_tokenizer": true,

    "input_column": "input",
    "target_column": null,
    "dataset_name": "0x70DA/chat-data",
    "dataset_config_name": null,
    "preprocessing_num_workers": 8,
    "max_source_length": 512,
    "pad_to_max_length": true,
    "ignore_pad_token_for_loss": true,
    "max_train_samples": 1000,
    "max_eval_samples": 50,

    "output_dir": "checkpoints/llama-7b-hf",
    "overwrite_output_dir": false,
    "evaluation_strategy": "steps",
    "per_device_train_batch_size": 8,
    "per_device_eval_batch_size": 8,
    "warmup_steps": 100,
    "num_train_epochs": 1,
    "learning_rate": 0.0002,
    "fp16": true,
    "logging_steps": 20,
    "save_strategy": "steps",
    "eval_steps": 200,
    "save_steps": 200,
    "save_total_limit": 100,
    "load_best_model_at_end": true,

    "r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.05,
    "target_modules": ["q_proj", "k_proj", "v_proj", "down_proj", "gate_proj", "up_proj"],
    "bias": "none",
    "task_type": "CAUSAL_LM"
}